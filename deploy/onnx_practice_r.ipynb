{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb060d4-b2da-4fa5-8838-b485ed2ba390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.15.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.17.1-cp38-cp38-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: numpy in /root/.local/lib/python3.8/site-packages (from onnx) (1.23.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /root/.local/lib/python3.8/site-packages (from onnx) (4.25.2)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-24.3.6-py2.py3-none-any.whl.metadata (849 bytes)\n",
      "Requirement already satisfied: packaging in /root/.local/lib/python3.8/site-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.12)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Downloading onnx-1.15.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading onnxruntime-1.17.1-cp38-cp38-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.6-py2.py3-none-any.whl (26 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: flatbuffers, onnx, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-24.3.6 humanfriendly-10.0 onnx-1.15.0 onnxruntime-1.17.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd799e6-ff31-4ca1-a946-dbd4e4360932",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e66c7bd-7b16-4ee6-9c56-dd9047673dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.model_zoo as model_zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792936e5-7d53-4a83-8413-88439ed6f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor, inplace=False):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv4.weight)\n",
    "\n",
    "# 위에서 정의된 모델을 사용하여 초해상도 모델 생성\n",
    "torch_model = SuperResolutionNet(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd913bf-4319-4d02-9e53-49b1eb9ef83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 학습된 가중치를 읽어옵니다\n",
    "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
    "batch_size = 1    # 임의의 수\n",
    "\n",
    "# 모델을 미리 학습된 가중치로 초기화합니다\n",
    "map_location = lambda storage, loc: storage\n",
    "if torch.cuda.is_available():\n",
    "    map_location = None\n",
    "torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n",
    "\n",
    "# 모델을 추론 모드로 전환합니다\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def09780-5d3d-4b16-a556-eaf8fecd48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
    "torch_out = torch_model(x)\n",
    "torch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165660cf-9db5-40b0-ac70-0fd4b861c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 변환\n",
    "torch.onnx.export(torch_model,               # 실행될 모델\n",
    "                  x,                         # 모델 입력값 (튜플 또는 여러 입력값들도 가능)\n",
    "                  \"./super_resolution_example.onnx\",   # 모델 저장 경로 (파일 또는 파일과 유사한 객체 모두 가능)\n",
    "                  export_params=True,        # 모델 파일 안에 학습된 모델 가중치를 저장할지의 여부\n",
    "                  opset_version=10,          # 모델을 변환할 때 사용할 ONNX 버전\n",
    "                  do_constant_folding=True,  # 최적화시 상수폴딩을 사용할지의 여부\n",
    "                  input_names = ['input'],   # 모델의 입력값을 가리키는 이름\n",
    "                  output_names = ['output'], # 모델의 출력값을 가리키는 이름\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # 가변적인 길이를 가진 차원\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0c6af-cc8f-4c38-b72c-1be53a0a7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import shape_inference\n",
    "path = \"../../OpenPCDet/deploy/super_resolution_example.onnx\"\n",
    "onnx.save(onnx.shape_inference.infer_shapes(onnx.load(path)), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b0b3e-ee7e-4fa7-a0be-f2f8b0006dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"super_resolution_example.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401b42f-d193-4f9c-a388-45f2170ad03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "ort_session = onnxruntime.InferenceSession(\"super_resolution_example.onnx\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# ONNX 런타임에서 계산된 결과값\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# ONNX 런타임과 PyTorch에서 연산된 결과값 비교\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c34b9-40dd-4800-ab71-437727210085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# img = Image.open(\"./_static/img/cat.jpg\")\n",
    "\n",
    "# resize = transforms.Resize([224, 224])\n",
    "# img = resize(img)\n",
    "\n",
    "# img_ycbcr = img.convert('YCbCr')\n",
    "# img_y, img_cb, img_cr = img_ycbcr.split()\n",
    "\n",
    "# to_tensor = transforms.ToTensor()\n",
    "# img_y = to_tensor(img_y)\n",
    "# img_y.unsqueeze_(0)\n",
    "\n",
    "# ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)}\n",
    "# ort_outs = ort_session.run(None, ort_inputs)\n",
    "# img_out_y = ort_outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d3e45-a506-4d18-b780-70a2457e9a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8eb38f1-3bdc-4e4c-a431-bd311236c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllDSVTBlocksTRT(nn.Module):\n",
    "    def __init__(self, dsvtblocks_list, layer_norms_list):\n",
    "        super().__init__()\n",
    "        self.layer_norms_list = layer_norms_list\n",
    "        self.dsvtblocks_list = dsvtblocks_list\n",
    "    def forward(\n",
    "        self,\n",
    "        pillar_features,\n",
    "        set_voxel_inds_tensor_shift_0,\n",
    "        set_voxel_inds_tensor_shift_1,\n",
    "        set_voxel_masks_tensor_shift_0,\n",
    "        set_voxel_masks_tensor_shift_1,\n",
    "        pos_embed_tensor,\n",
    "    ):\n",
    "        outputs = pillar_features\n",
    "\n",
    "        residual = outputs\n",
    "        blc_id = 0\n",
    "        set_id = 0\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "        set_id = 1\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "\n",
    "        outputs = self.layer_norms_list[blc_id](residual + outputs)\n",
    "\n",
    "        residual = outputs\n",
    "        blc_id = 1\n",
    "        set_id = 0\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "        set_id = 1\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "\n",
    "        outputs = self.layer_norms_list[blc_id](residual + outputs)\n",
    "\n",
    "        residual = outputs\n",
    "        blc_id = 2\n",
    "        set_id = 0\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "        set_id = 1\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "\n",
    "        outputs = self.layer_norms_list[blc_id](residual + outputs)\n",
    "\n",
    "        residual = outputs\n",
    "        blc_id = 3\n",
    "        set_id = 0\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "        set_id = 1\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "\n",
    "        outputs = self.layer_norms_list[blc_id](residual + outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a3e03-328a-460c-ab00-b8352f0948c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllDSVTBlocksTRT(nn.Module):\n",
    "    def __init__(self, dsvtblocks_list, layer_norms_list):\n",
    "        super().__init__()\n",
    "        self.layer_norms_list = layer_norms_list\n",
    "        self.dsvtblocks_list = dsvtblocks_list\n",
    "    def forward(\n",
    "        self,\n",
    "        pillar_features,\n",
    "        set_voxel_inds_tensor_shift_0,\n",
    "        set_voxel_inds_tensor_shift_1,\n",
    "        set_voxel_masks_tensor_shift_0,\n",
    "        set_voxel_masks_tensor_shift_1,\n",
    "        pos_embed_tensor,\n",
    "    ):\n",
    "        outputs = pillar_features\n",
    "\n",
    "        residual = outputs\n",
    "        blc_id = 0\n",
    "        set_id = 0\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "        set_id = 1\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "\n",
    "        outputs = self.layer_norms_list[blc_id](residual + outputs)\n",
    "\n",
    "        residual = outputs\n",
    "        blc_id = 1\n",
    "        set_id = 0\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "        set_id = 1\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "\n",
    "        outputs = self.layer_norms_list[blc_id](residual + outputs)\n",
    "\n",
    "        residual = outputs\n",
    "        blc_id = 2\n",
    "        set_id = 0\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "        set_id = 1\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_0[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "\n",
    "        outputs = self.layer_norms_list[blc_id](residual + outputs)\n",
    "\n",
    "        residual = outputs\n",
    "        blc_id = 3\n",
    "        set_id = 0\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "        set_id = 1\n",
    "        set_voxel_inds = set_voxel_inds_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        set_voxel_masks = set_voxel_masks_tensor_shift_1[set_id:set_id+1].squeeze(0)\n",
    "        pos_embed = pos_embed_tensor[blc_id:blc_id+1, set_id:set_id+1].squeeze(0).squeeze(0)\n",
    "        inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n",
    "        # inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\n",
    "        outputs = self.dsvtblocks_list[blc_id].encoder_list[set_id](*inputs)\n",
    "\n",
    "        outputs = self.layer_norms_list[blc_id](residual + outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c542af-a261-4d32-861f-7778e26e5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.config import cfg, cfg_from_yaml_file\n",
    "from pcdet.models import build_network\n",
    "from pcdet.datasets import build_dataloader\n",
    "from pcdet.utils import common_utils\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "# # import onnx\n",
    "import onnxruntime as ort\n",
    "import torch.nn as nn\n",
    "\n",
    "# from typing import Sequence, NamedTuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0881cd-682a-4f5a-9a29-e72f88e78f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 13:02:14,895   INFO  Loading Waymo dataset\n",
      "2024-03-07 13:02:18,125   INFO  Total skipped info 0\n",
      "2024-03-07 13:02:18,126   INFO  Total samples for Waymo dataset: 39987\n"
     ]
    }
   ],
   "source": [
    "# cfg_file = \"./onnx_config.yaml\"\n",
    "# cfg_file = \"../tools/cfgs/waymo_models/dsvt_pillar.yaml\"\n",
    "cfg_file = \"../tools/cfgs/waymo_models/dsvt_voxel.yaml\"\n",
    "\n",
    "cfg_from_yaml_file(cfg_file, cfg)\n",
    "if os.path.exists('./deploy_files')==False:\n",
    "    os.mkdir('./deploy_files')\n",
    "log_file = './deploy_files/log_trt.log'\n",
    "logger = common_utils.create_logger(log_file, rank=0)\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    class_names=cfg.CLASS_NAMES,\n",
    "    batch_size=1,\n",
    "    dist=False, workers=8, logger=logger, training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c4e51bf-24f9-4c53-90df-a450d8a4fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4d44ba-8ac3-4405-b1cc-a65efffc5eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 13:02:20,278   INFO  ==> Loading parameters from checkpoint /mnt/nas2/users/eslim/result_log/generalization/dsvt_voxel_240225_r/ckpt/latest_model.pth to GPU\n",
      "2024-03-07 13:02:20,419   INFO  ==> Checkpoint trained from version: pcdet+0.6.0+6719625+pyf948e89\n",
      "2024-03-07 13:02:20,453   INFO  Not updated weight vfe.pfn_layers.0.linear.weight: torch.Size([96, 11])\n",
      "2024-03-07 13:02:20,454   INFO  ==> Done (loaded 411/412)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CenterPoint(\n",
       "  (vfe): DynamicVoxelVFE(\n",
       "    (pfn_layers): ModuleList(\n",
       "      (0): PFNLayerV2(\n",
       "        (linear): Linear(in_features=11, out_features=96, bias=False)\n",
       "        (norm): BatchNorm1d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): PFNLayerV2(\n",
       "        (linear): Linear(in_features=192, out_features=192, bias=False)\n",
       "        (norm): BatchNorm1d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (backbone_3d): DSVT(\n",
       "    (input_layer): DSVTInputLayer(\n",
       "      (posembed_layers): ModuleList(\n",
       "        (0-2): 3 x ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0-1): 2 x PositionEmbeddingLearned(\n",
       "              (position_embedding_head): Sequential(\n",
       "                (0): Linear(in_features=3, out_features=192, bias=True)\n",
       "                (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0-1): 2 x PositionEmbeddingLearned(\n",
       "              (position_embedding_head): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=192, bias=True)\n",
       "                (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage_0): ModuleList(\n",
       "      (0): DSVTBlock(\n",
       "        (encoder_list): ModuleList(\n",
       "          (0-1): 2 x DSVT_EncoderLayer(\n",
       "            (win_attn): SetAttention(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=192, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "              (linear2): Linear(in_features=384, out_features=192, bias=True)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Identity()\n",
       "              (dropout2): Identity()\n",
       "            )\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (residual_norm_stage_0): ModuleList(\n",
       "      (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (stage_0_reduction): Stage_ReductionAtt_Block(\n",
       "      (query_func): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (stage_1): ModuleList(\n",
       "      (0): DSVTBlock(\n",
       "        (encoder_list): ModuleList(\n",
       "          (0-1): 2 x DSVT_EncoderLayer(\n",
       "            (win_attn): SetAttention(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=192, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "              (linear2): Linear(in_features=384, out_features=192, bias=True)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Identity()\n",
       "              (dropout2): Identity()\n",
       "            )\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (residual_norm_stage_1): ModuleList(\n",
       "      (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (stage_1_reduction): Stage_ReductionAtt_Block(\n",
       "      (query_func): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (stage_2): ModuleList(\n",
       "      (0): DSVTBlock(\n",
       "        (encoder_list): ModuleList(\n",
       "          (0-1): 2 x DSVT_EncoderLayer(\n",
       "            (win_attn): SetAttention(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=192, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "              (linear2): Linear(in_features=384, out_features=192, bias=True)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Identity()\n",
       "              (dropout2): Identity()\n",
       "            )\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (residual_norm_stage_2): ModuleList(\n",
       "      (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (stage_2_reduction): Stage_ReductionAtt_Block(\n",
       "      (query_func): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (stage_3): ModuleList(\n",
       "      (0): DSVTBlock(\n",
       "        (encoder_list): ModuleList(\n",
       "          (0-1): 2 x DSVT_EncoderLayer(\n",
       "            (win_attn): SetAttention(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=192, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "              (linear2): Linear(in_features=384, out_features=192, bias=True)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Identity()\n",
       "              (dropout2): Identity()\n",
       "            )\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (residual_norm_stage_3): ModuleList(\n",
       "      (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (map_to_bev_module): PointPillarScatter3d()\n",
       "  (pfe): None\n",
       "  (backbone_2d): BaseBEVResBackbone(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU()\n",
       "          (downsample_layer): Sequential(\n",
       "            (0): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU()\n",
       "          (downsample_layer): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU()\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU()\n",
       "          (downsample_layer): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU()\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (deblocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense_head): CenterHead(\n",
       "    (shared_conv): Sequential(\n",
       "      (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (heads_list): ModuleList(\n",
       "      (0): SeparateHead(\n",
       "        (center): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (center_z): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (dim): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (rot): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (iou): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (hm): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (hm_loss_func): FocalLossCenterNet()\n",
       "    (reg_loss_func): RegLossCenterNet()\n",
       "  )\n",
       "  (point_head): None\n",
       "  (roi_head): None\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckpt = \"/mnt/nas2/users/eslim/result_log/dsvt_pillar_waymo/ckpt/latest_model.pth\"\n",
    "ckpt = \"/mnt/nas2/users/eslim/result_log/generalization/dsvt_voxel_240225_r/ckpt/latest_model.pth\"\n",
    "model.load_params_from_file(filename=ckpt, logger=logger, to_cpu=False, pre_trained_path=None)\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "140fc7f9-9d22-4a76-b68a-dffbd88ff25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#     DSVT_Backbone = model.backbone_3d\n",
    "#     dsvtblocks_list = DSVT_Backbone.stage_0\n",
    "#     layer_norms_list = DSVT_Backbone.residual_norm_stage_0\n",
    "#     inputs = model.vfe(inputs)\n",
    "#     voxel_info = DSVT_Backbone.input_layer(inputs)\n",
    "#     set_voxel_inds_list = [[voxel_info[f'set_voxel_inds_stage{s}_shift{i}'] for i in range(2)] for s in range(1)]\n",
    "#     set_voxel_masks_list = [[voxel_info[f'set_voxel_mask_stage{s}_shift{i}'] for i in range(2)] for s in range(1)]\n",
    "#     pos_embed_list = [[[voxel_info[f'pos_embed_stage{s}_block{b}_shift{i}'] for i in range(2)] for b in range(4)] for s in range(1)]\n",
    "# inputs['voxel_features'].size()\n",
    "# # print(len(set_voxel_masks_list))\n",
    "# # pos_embed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8cf3b86-6ae4-4cd8-a1b7-89b1958dac59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  ...,  True,  True,  True],\n",
       "         [False,  True,  True,  ...,  True,  True,  True],\n",
       "         [False,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False,  True, False,  ..., False,  True, False],\n",
       "         [False,  True,  True,  ...,  True,  True,  True],\n",
       "         [False,  True,  True,  ...,  True,  True,  True]],\n",
       "\n",
       "        [[False,  True,  True,  ...,  True,  True,  True],\n",
       "         [False,  True,  True,  ...,  True,  True,  True],\n",
       "         [False,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False,  True, False,  ..., False,  True, False],\n",
       "         [False,  True,  True,  ...,  True,  True,  True],\n",
       "         [False,  True,  True,  ...,  True,  True,  True]]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set_voxel_masks_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b55125b8-e0ad-4eff-87af-ad9d326f0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Haiyang-W/DSVT/issues/28\n",
    "# https://drive.google.com/file/d/1AimmC2Fc-40AyK-xM1D_fGV09uXrrsgS/view\n",
    "batch_dict = torch.load(\"./batch_dict.pth\", map_location=\"cuda\")\n",
    "inputs = batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d2c2f28-6cba-4ad7-a70b-dd84041d53ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['voxel_feats_stage0', 'voxel_coors_stage0', 'batch_win_inds_stage0_shift0', 'coors_in_win_stage0_shift0', 'batch_win_inds_stage0_shift1', 'coors_in_win_stage0_shift1', 'set_voxel_inds_stage0_shift0', 'set_voxel_mask_stage0_shift0', 'set_voxel_inds_stage0_shift1', 'set_voxel_mask_stage0_shift1', 'pos_embed_stage0_block0_shift0', 'pos_embed_stage0_block0_shift1', 'pooling_mapping_index_stage1', 'pooling_index_in_pool_stage1', 'pooling_preholder_feats_stage1', 'voxel_coors_stage1', 'batch_win_inds_stage1_shift0', 'coors_in_win_stage1_shift0', 'batch_win_inds_stage1_shift1', 'coors_in_win_stage1_shift1', 'set_voxel_inds_stage1_shift0', 'set_voxel_mask_stage1_shift0', 'set_voxel_inds_stage1_shift1', 'set_voxel_mask_stage1_shift1', 'pos_embed_stage1_block0_shift0', 'pos_embed_stage1_block0_shift1', 'pooling_mapping_index_stage2', 'pooling_index_in_pool_stage2', 'pooling_preholder_feats_stage2', 'voxel_coors_stage2', 'batch_win_inds_stage2_shift0', 'coors_in_win_stage2_shift0', 'batch_win_inds_stage2_shift1', 'coors_in_win_stage2_shift1', 'set_voxel_inds_stage2_shift0', 'set_voxel_mask_stage2_shift0', 'set_voxel_inds_stage2_shift1', 'set_voxel_mask_stage2_shift1', 'pos_embed_stage2_block0_shift0', 'pos_embed_stage2_block0_shift1', 'pooling_mapping_index_stage3', 'pooling_index_in_pool_stage3', 'pooling_preholder_feats_stage3', 'voxel_coors_stage3', 'batch_win_inds_stage3_shift0', 'coors_in_win_stage3_shift0', 'batch_win_inds_stage3_shift1', 'coors_in_win_stage3_shift1', 'set_voxel_inds_stage3_shift0', 'set_voxel_mask_stage3_shift0', 'set_voxel_inds_stage3_shift1', 'set_voxel_mask_stage3_shift1', 'pos_embed_stage3_block0_shift0', 'pos_embed_stage3_block0_shift1'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxel_info.keys()\n",
    "# dict_keys(['voxel_feats_sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53498833-0927-4e9a-9030-6a64634e42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    DSVT_Backbone = model.backbone_3d\n",
    "    dsvtblocks_list = DSVT_Backbone.stage_0\n",
    "    layer_norms_list = DSVT_Backbone.residual_norm_stage_0\n",
    "    reduction_list = DSVT_Backbone.stage_0_reduction\n",
    "    inputs = model.vfe(inputs)\n",
    "    voxel_info = DSVT_Backbone.input_layer(inputs)\n",
    "    set_voxel_inds_list = [[voxel_info[f'set_voxel_inds_stage{s}_shift{i}'] for i in range(2)] for s in range(4)]\n",
    "    set_voxel_masks_list = [[voxel_info[f'set_voxel_mask_stage{s}_shift{i}'] for i in range(2)] for s in range(4)]\n",
    "    pos_embed_list = [[[voxel_info[f'pos_embed_stage{s}_block{b}_shift{i}'] for i in range(2)] for b in range(1)] for s in range(4)]\n",
    "\n",
    "    voxel_features = inputs['voxel_features']\n",
    "    alldsvtblockstrt_inputs = (\n",
    "        pillar_features,\n",
    "        set_voxel_inds_list[0][0],\n",
    "        set_voxel_inds_list[0][1],\n",
    "        set_voxel_masks_list[0][0],\n",
    "        set_voxel_masks_list[0][1],\n",
    "        torch.stack([torch.stack(v, dim=0) for v in pos_embed_list[0]], dim=0),\n",
    "    )\n",
    "# voxel_info.keys()\n",
    "# dict_keys(['voxel_feats_stage0', 'voxel_coors_stage0', 'batch_win_inds_stage0_shift0', 'coors_in_win_stage0_shift0', 'batch_win_inds_stage0_shift1', 'coors_in_win_stage0_shift1', 'set_voxel_inds_stage0_shift0', 'set_voxel_mask_stage0_shift0', 'set_voxel_inds_stage0_shift1', 'set_voxel_mask_stage0_shift1', 'pos_embed_stage0_block0_shift0', 'pos_embed_stage0_block0_shift1', 'pooling_mapping_index_stage1', 'pooling_index_in_pool_stage1', 'pooling_preholder_feats_stage1', 'voxel_coors_stage1', 'batch_win_inds_stage1_shift0', 'coors_in_win_stage1_shift0', 'batch_win_inds_stage1_shift1', 'coors_in_win_stage1_shift1', 'set_voxel_inds_stage1_shift0', 'set_voxel_mask_stage1_shift0', 'set_voxel_inds_stage1_shift1', 'set_voxel_mask_stage1_shift1', 'pos_embed_stage1_block0_shift0', 'pos_embed_stage1_block0_shift1', 'pooling_mapping_index_stage2', 'pooling_index_in_pool_stage2', 'pooling_preholder_feats_stage2', 'voxel_coors_stage2', 'batch_win_inds_stage2_shift0', 'coors_in_win_stage2_shift0', 'batch_win_inds_stage2_shift1', 'coors_in_win_stage2_shift1', 'set_voxel_inds_stage2_shift0', 'set_voxel_mask_stage2_shift0', 'set_voxel_inds_stage2_shift1', 'set_voxel_mask_stage2_shift1', 'pos_embed_stage2_block0_shift0', 'pos_embed_stage2_block0_shift1', 'pooling_mapping_index_stage3', 'pooling_index_in_pool_stage3', 'pooling_preholder_feats_stage3', 'voxel_coors_stage3', 'batch_win_inds_stage3_shift0', 'coors_in_win_stage3_shift0', 'batch_win_inds_stage3_shift1', 'coors_in_win_stage3_shift1', 'set_voxel_inds_stage3_shift0', 'set_voxel_mask_stage3_shift0', 'set_voxel_inds_stage3_shift1', 'set_voxel_mask_stage3_shift1', 'pos_embed_stage3_block0_shift0', 'pos_embed_stage3_block0_shift1'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d085759-0c32-4d07-bb51-6dc6598ec597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jit_mode = \"trace\"\n",
    "input_names = [\n",
    "    'src',\n",
    "    'set_voxel_inds_tensor_shift_0',\n",
    "    'set_voxel_inds_tensor_shift_1',\n",
    "    'set_voxel_masks_tensor_shift_0',\n",
    "    'set_voxel_masks_tensor_shift_1',\n",
    "    'pos_embed_tensor'\n",
    "]\n",
    "output_names = [\"output\",]\n",
    "input_shapes = {\n",
    "    \"src\": {\n",
    "        \"min_shape\": [24629, 192],\n",
    "        \"opt_shape\": [24629, 192],\n",
    "        \"max_shape\": [24629, 192],\n",
    "    },\n",
    "    \"set_voxel_inds_tensor_shift_0\": {\n",
    "        \"min_shape\": [2, 1156, 36],\n",
    "        \"opt_shape\": [2, 1156, 36],\n",
    "        \"max_shape\": [2, 1156, 36],\n",
    "    },\n",
    "    \"set_voxel_inds_tensor_shift_1\": {\n",
    "        \"min_shape\": [2, 834, 36],\n",
    "        \"opt_shape\": [2, 834, 36],\n",
    "        \"max_shape\": [2, 834, 36],\n",
    "    },\n",
    "    \"set_voxel_masks_tensor_shift_0\": {\n",
    "        \"min_shape\": [2, 1156, 36],\n",
    "        \"opt_shape\": [2, 1156, 36],\n",
    "        \"max_shape\": [2, 1156, 36],\n",
    "    },\n",
    "    \"set_voxel_masks_tensor_shift_1\": {\n",
    "        \"min_shape\": [2, 834, 36],\n",
    "        \"opt_shape\": [2, 834, 36],\n",
    "        \"max_shape\": [2, 834, 36],\n",
    "    },\n",
    "    \"pos_embed_tensor\": {\n",
    "        \"min_shape\": [4, 2, 24629, 192],\n",
    "        \"opt_shape\": [4, 2, 24629, 192],\n",
    "        \"max_shape\": [4, 2, 24629, 192],\n",
    "    },\n",
    "}\n",
    "\n",
    "dynamic_axes = {\n",
    "    \"src\": {\n",
    "        0: \"voxel_number\",\n",
    "    },\n",
    "    \"set_voxel_inds_tensor_shift_0\": {\n",
    "        1: \"set_number_shift_0\",\n",
    "    },\n",
    "    \"set_voxel_inds_tensor_shift_1\": {\n",
    "        1: \"set_number_shift_1\",\n",
    "    },\n",
    "    \"set_voxel_masks_tensor_shift_0\": {\n",
    "        1: \"set_number_shift_0\",\n",
    "    },\n",
    "    \"set_voxel_masks_tensor_shift_1\": {\n",
    "        1: \"set_number_shift_1\",\n",
    "    },\n",
    "    \"pos_embed_tensor\": {\n",
    "        2: \"voxel_number\",\n",
    "    },\n",
    "    \"output\": {\n",
    "        0: \"voxel_number\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "141f4d3a-a276-425d-a7f1-6e3d684df573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m onnx_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m allptransblocktrt \u001b[38;5;241m=\u001b[39m AllDSVTBlocksTRT(dsvtblocks_list, layer_norms_list)\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallptransblocktrt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43malldsvtblockstrt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# https://github.com/Haiyang-W/DSVT/issues/60\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:506\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    190\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     export_modules_as_functions: Union[\u001b[38;5;28mbool\u001b[39m, Collection[Type[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1548\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1546\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1548\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1563\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1564\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1113\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m   1112\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1113\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:989\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    984\u001b[0m     graph \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m    985\u001b[0m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     )\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m graph, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    991\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:893\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    891\u001b[0m prev_autocast_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    892\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 893\u001b[0m trace_graph, torch_out, inputs_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    902\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py:1268\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1267\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m-> 1268\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mONNXTracedModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1488\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m, in \u001b[0;36mAllDSVTBlocksTRT.forward\u001b[0;34m(self, pillar_features, set_voxel_inds_tensor_shift_0, set_voxel_inds_tensor_shift_1, set_voxel_masks_tensor_shift_0, set_voxel_masks_tensor_shift_1, pos_embed_tensor)\u001b[0m\n\u001b[1;32m     44\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (outputs, set_voxel_inds, set_voxel_masks, pos_embed)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# inputs = (outputs, set_voxel_inds, set_voxel_masks, pos_embed, True)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdsvtblocks_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblc_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mencoder_list[set_id](\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     48\u001b[0m set_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m set_voxel_inds \u001b[38;5;241m=\u001b[39m set_voxel_inds_tensor_shift_1[set_id:set_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:295\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())[idx])\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:285\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    283\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx))\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    287\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of range"
     ]
    }
   ],
   "source": [
    "base_name = \"./deploy_files/dsvt\"\n",
    "ts_path = f\"{base_name}.ts\"\n",
    "onnx_path = f\"{base_name}.onnx\"\n",
    "\n",
    "allptransblocktrt = AllDSVTBlocksTRT(dsvtblocks_list, layer_norms_list).eval().cuda()\n",
    "\n",
    "torch.onnx.export(\n",
    "    allptransblocktrt,\n",
    "    alldsvtblockstrt_inputs,\n",
    "    onnx_path, input_names=input_names,\n",
    "    output_names=output_names, dynamic_axes=dynamic_axes,\n",
    "    opset_version=14,\n",
    ")\n",
    "# https://github.com/Haiyang-W/DSVT/issues/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e761c40-ca7c-4874-9740-86c0793f8c85",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pos_embed_stage0_block1_shift0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m set_voxel_inds_list \u001b[38;5;241m=\u001b[39m [[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_voxel_inds_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      8\u001b[0m set_voxel_masks_list \u001b[38;5;241m=\u001b[39m [[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_voxel_mask_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m----> 9\u001b[0m pos_embed_list \u001b[38;5;241m=\u001b[39m [[[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_embed_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_block\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     11\u001b[0m pillar_features \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoxel_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m alldsvtblockstrt_inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     13\u001b[0m     pillar_features,\n\u001b[1;32m     14\u001b[0m     set_voxel_inds_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mstack(v, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pos_embed_list[\u001b[38;5;241m0\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     19\u001b[0m )\n",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m set_voxel_inds_list \u001b[38;5;241m=\u001b[39m [[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_voxel_inds_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      8\u001b[0m set_voxel_masks_list \u001b[38;5;241m=\u001b[39m [[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_voxel_mask_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m----> 9\u001b[0m pos_embed_list \u001b[38;5;241m=\u001b[39m [[[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_embed_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_block\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     11\u001b[0m pillar_features \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoxel_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m alldsvtblockstrt_inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     13\u001b[0m     pillar_features,\n\u001b[1;32m     14\u001b[0m     set_voxel_inds_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mstack(v, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pos_embed_list[\u001b[38;5;241m0\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     19\u001b[0m )\n",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m set_voxel_inds_list \u001b[38;5;241m=\u001b[39m [[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_voxel_inds_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      8\u001b[0m set_voxel_masks_list \u001b[38;5;241m=\u001b[39m [[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_voxel_mask_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m----> 9\u001b[0m pos_embed_list \u001b[38;5;241m=\u001b[39m [[[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_embed_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_block\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     11\u001b[0m pillar_features \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoxel_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m alldsvtblockstrt_inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     13\u001b[0m     pillar_features,\n\u001b[1;32m     14\u001b[0m     set_voxel_inds_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mstack(v, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pos_embed_list[\u001b[38;5;241m0\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     19\u001b[0m )\n",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m set_voxel_inds_list \u001b[38;5;241m=\u001b[39m [[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_voxel_inds_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      8\u001b[0m set_voxel_masks_list \u001b[38;5;241m=\u001b[39m [[voxel_info[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_voxel_mask_stage\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shift\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m----> 9\u001b[0m pos_embed_list \u001b[38;5;241m=\u001b[39m [[[\u001b[43mvoxel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos_embed_stage\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ms\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_block\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mb\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_shift\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     11\u001b[0m pillar_features \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoxel_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m alldsvtblockstrt_inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     13\u001b[0m     pillar_features,\n\u001b[1;32m     14\u001b[0m     set_voxel_inds_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mstack(v, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pos_embed_list[\u001b[38;5;241m0\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pos_embed_stage0_block1_shift0'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    DSVT_Backbone = model.backbone_3d\n",
    "    dsvtblocks_list = DSVT_Backbone.stage_0\n",
    "    layer_norms_list = DSVT_Backbone.residual_norm_stage_0\n",
    "    inputs = model.vfe(inputs)\n",
    "    voxel_info = DSVT_Backbone.input_layer(inputs)\n",
    "    set_voxel_inds_list = [[voxel_info[f'set_voxel_inds_stage{s}_shift{i}'] for i in range(2)] for s in range(1)]\n",
    "    set_voxel_masks_list = [[voxel_info[f'set_voxel_mask_stage{s}_shift{i}'] for i in range(2)] for s in range(1)]\n",
    "    pos_embed_list = [[[voxel_info[f'pos_embed_stage{s}_block{b}_shift{i}'] for i in range(2)] for b in range(4)] for s in range(1)]\n",
    "\n",
    "    pillar_features = inputs['voxel_features']\n",
    "    alldsvtblockstrt_inputs = (\n",
    "        pillar_features,\n",
    "        set_voxel_inds_list[0][0],\n",
    "        set_voxel_inds_list[0][1],\n",
    "        set_voxel_masks_list[0][0],\n",
    "        set_voxel_masks_list[0][1],\n",
    "        torch.stack([torch.stack(v, dim=0) for v in pos_embed_list[0]], dim=0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965b7ec-ef9a-4698-bcc7-d5fb1c04885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a17056-8a30-421b-b4d4-34d44560f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jit_mode = \"trace\"\n",
    "input_names = [\n",
    "    'src',\n",
    "    'set_voxel_inds_tensor_shift_0',\n",
    "    'set_voxel_inds_tensor_shift_1',\n",
    "    'set_voxel_masks_tensor_shift_0',\n",
    "    'set_voxel_masks_tensor_shift_1',\n",
    "    'pos_embed_tensor'\n",
    "]\n",
    "output_names = [\"output\",]\n",
    "input_shapes = {\n",
    "    \"src\": {\n",
    "        \"min_shape\": [24629, 192],\n",
    "        \"opt_shape\": [24629, 192],\n",
    "        \"max_shape\": [24629, 192],\n",
    "    },\n",
    "    \"set_voxel_inds_tensor_shift_0\": {\n",
    "        \"min_shape\": [2, 1156, 36],\n",
    "        \"opt_shape\": [2, 1156, 36],\n",
    "        \"max_shape\": [2, 1156, 36],\n",
    "    },\n",
    "    \"set_voxel_inds_tensor_shift_1\": {\n",
    "        \"min_shape\": [2, 834, 36],\n",
    "        \"opt_shape\": [2, 834, 36],\n",
    "        \"max_shape\": [2, 834, 36],\n",
    "    },\n",
    "    \"set_voxel_masks_tensor_shift_0\": {\n",
    "        \"min_shape\": [2, 1156, 36],\n",
    "        \"opt_shape\": [2, 1156, 36],\n",
    "        \"max_shape\": [2, 1156, 36],\n",
    "    },\n",
    "    \"set_voxel_masks_tensor_shift_1\": {\n",
    "        \"min_shape\": [2, 834, 36],\n",
    "        \"opt_shape\": [2, 834, 36],\n",
    "        \"max_shape\": [2, 834, 36],\n",
    "    },\n",
    "    \"pos_embed_tensor\": {\n",
    "        \"min_shape\": [4, 2, 24629, 192],\n",
    "        \"opt_shape\": [4, 2, 24629, 192],\n",
    "        \"max_shape\": [4, 2, 24629, 192],\n",
    "    },\n",
    "}\n",
    "\n",
    "dynamic_axes = {\n",
    "    \"src\": {\n",
    "        0: \"voxel_number\",\n",
    "    },\n",
    "    \"set_voxel_inds_tensor_shift_0\": {\n",
    "        1: \"set_number_shift_0\",\n",
    "    },\n",
    "    \"set_voxel_inds_tensor_shift_1\": {\n",
    "        1: \"set_number_shift_1\",\n",
    "    },\n",
    "    \"set_voxel_masks_tensor_shift_0\": {\n",
    "        1: \"set_number_shift_0\",\n",
    "    },\n",
    "    \"set_voxel_masks_tensor_shift_1\": {\n",
    "        1: \"set_number_shift_1\",\n",
    "    },\n",
    "    \"pos_embed_tensor\": {\n",
    "        2: \"voxel_number\",\n",
    "    },\n",
    "    \"output\": {\n",
    "        0: \"voxel_number\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56bf0819-7282-4712-b674-16fbde7fa159",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = \"./deploy_files/dsvt\"\n",
    "ts_path = f\"{base_name}.ts\"\n",
    "onnx_path = f\"{base_name}.onnx\"\n",
    "\n",
    "allptransblocktrt = AllDSVTBlocksTRT(dsvtblocks_list, layer_norms_list).eval().cuda()\n",
    "\n",
    "torch.onnx.export(\n",
    "    allptransblocktrt,\n",
    "    alldsvtblockstrt_inputs,\n",
    "    onnx_path, input_names=input_names,\n",
    "    output_names=output_names, dynamic_axes=dynamic_axes,\n",
    "    opset_version=14,\n",
    ")\n",
    "# https://github.com/Haiyang-W/DSVT/issues/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6236ae32-6c26-4c93-823f-077a2cfb37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test onnx\n",
    "ort_session = ort.InferenceSession(onnx_path)\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(pillar_features),\n",
    "              ort_session.get_inputs()[1].name: to_numpy(set_voxel_inds_list[0][0]),\n",
    "              ort_session.get_inputs()[2].name: to_numpy(set_voxel_inds_list[0][1]),\n",
    "              ort_session.get_inputs()[3].name: to_numpy(set_voxel_masks_list[0][0]),\n",
    "              ort_session.get_inputs()[4].name: to_numpy(set_voxel_masks_list[0][1]),\n",
    "              ort_session.get_inputs()[5].name: to_numpy(torch.stack([torch.stack(v, dim=0) for v in pos_embed_list[0]], dim=0)),}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "# ort_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5fa9195-b299-485f-ad50-20f6a9a82121",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trt##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f5013fd-99e6-4da3-9410-9637087d74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -m pip install --upgrade tensorrt\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc14eda1-9db8-47b4-a834-6fc59444d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !trtexec --onnx=./deploy_files/dsvt.onnx  --saveEngine=./deploy_files/dsvt.engine \\\n",
    "# --memPoolSize=workspace:4096 --verbose --buildOnly --device=1 --fp16 \\\n",
    "# --tacticSources=+CUDNN,+CUBLAS,-CUBLAS_LT,+EDGE_MASK_CONVOLUTIONS \\\n",
    "# --minShapes=src:3000x192,set_voxel_inds_tensor_shift_0:2x170x36,set_voxel_inds_tensor_shift_1:2x100x36,set_voxel_masks_tensor_shift_0:2x170x36,set_voxel_masks_tensor_shift_1:2x100x36,pos_embed_tensor:4x2x3000x192 \\\n",
    "# --optShapes=src:20000x192,set_voxel_inds_tensor_shift_0:2x1000x36,set_voxel_inds_tensor_shift_1:2x700x36,set_voxel_masks_tensor_shift_0:2x1000x36,set_voxel_masks_tensor_shift_1:2x700x36,pos_embed_tensor:4x2x20000x192 \\\n",
    "# --maxShapes=src:35000x192,set_voxel_inds_tensor_shift_0:2x1500x36,set_voxel_inds_tensor_shift_1:2x1200x36,set_voxel_masks_tensor_shift_0:2x1500x36,set_voxel_masks_tensor_shift_1:2x1200x36,pos_embed_tensor:4x2x35000x192 \\\n",
    "# > debug.log 2>&1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
